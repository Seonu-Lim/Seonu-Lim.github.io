---
layout: single
classes: wide
title: "공공데이터포털 API 사용하기"
categories:
  - Python
last_modified_at: 2020-08-14T13:00:00+09:00
use_math: false
---

내가 다니게 된 스타트업은 사실 데이터도 다 못모은 슬프고 작은 회사기 때문에 내가 딥러닝을 하기 위해서는 API 를 직접 긁어와야 한다. 그런데 [공공데이터포털](https://www.data.go.kr/)의 데이터를 가져올 때 또 좀 헤맸다. 왜냐하면 데이터포털에서 python 예시 코드라고 올려둔 것이 python2 코드였기 때문이다. 다음부터는 안 헤매기 위해 코드를 블로그에 적어둔다.

Parsing을 위해 BeautifulSoup 를 사용하였다. 코드를 복붙해 실행하기 전에 꼭 `pip install beautifulsoup4` 를 터미널에서 실행하시길...

```python
from urllib.request import Request,urlopen
from urllib.parse import urlencode, quote_plus, unquote
import pandas as pd
from bs4 import BeautifulSoup

decoded = unquote('API 사용신청하고 받은 일반인증키를 여기에 입력')
url = '사용설명서에 써 있는 서비스 URL을 여기에 복붙'

queryParams = '?' + urlencode({ quote_plus('serviceKey') : decoded ,
                               quote_plus('numOfRows') : '999',
                               quote_plus('pageNo') : '1',
                               quote_plus('input_var1') : 'blahblah',
                               quote_plus('input_var2') : 'blahblah',
                               quote_plus('input_var3') : 'blahblah',
                               quote_plus('input_var4') : 'blahblah',
                               quote_plus('input_var5') : 'blahblah' })

request = Request(url + queryParams)
request.get_method = lambda: 'GET'
response_body += urlopen(request).read()

#페이지가 많을 경우 다음과 같이 loop
i = 2
while 1 :
    page = str(i)
    queryParams = '?' + urlencode({ quote_plus('serviceKey') : decoded ,
                               quote_plus('numOfRows') : '999',
                               quote_plus('pageNo') : page,
                               quote_plus('input_var1') : 'blahblah',
                               quote_plus('input_var2') : 'blahblah',
                               quote_plus('input_var3') : 'blahblah',
                               quote_plus('input_var4') : 'blahblah',
                               quote_plus('input_var5') : 'blahblah' })
    request = Request(url + queryParams)
    request.get_method = lambda: 'GET'
    if b'NO_DATA' in response_body :
      print('All Done.')
      break
    else :
      response_body += urlopen(request).read()
      print('Page '+page+' Done.')
      i += 1

bulk = BeautifulSoup(response_body,'html.parser')
```
input_var1~5와 blahblah 는 설명서를 참고해서 url 주소의 구성요소를 채우면 된다.
여기까지 하면 가져오고 싶은 xml 을 binary-like object 한개로 합쳐서 가져오게 된다. 이렇게 받은 데이터를 list 로 저장하려면,

```python
data = []
for item in bulk.find_all('item') :
  d = item.find('찾고 싶은 변수의 항목명(설명서 참고)')
  if d is not None :
    data.append(d.string)
  else :
    data.append('NA')
```
이렇게 하면 단일 변수 하나가 완성된다. 주의할 점은, 설명서의 변수는 대문자들을 포함하고 있지만 내가 API 두개를 가져와 보니 실제로는 다 소문자로 입력해야 한다는 것이다. 데이터마다 다를 수 있으니 이것은 직접 데이터를 한 페이지 출력시켜서 확인해 보는 것이 좋겠다.
